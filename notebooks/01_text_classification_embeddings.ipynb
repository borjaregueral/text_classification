{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# %reload_ext autoreload\n","# %autoreload 2"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, LSTM, Dense, Dropout\n","from tensorflow.keras.regularizers import l2\n","from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n","from keras_tuner import BayesianOptimization\n","import numpy as np\n","from sklearn.utils.class_weight import compute_class_weight\n","from tensorflow.keras.layers import (\n","    Embedding, Conv1D, MaxPooling1D, LSTM, Dense, Dropout, Bidirectional, GRU\n",")\n","import plotly.graph_objects as go\n","from sklearn.metrics import roc_curve, auc\n","from sklearn.model_selection import train_test_split\n","\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","from text_classification.paths import DATA_DIR, MODELS_DIR\n","\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, LSTM, Dense, Dropout\n","from tensorflow.keras.regularizers import l2\n","from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n","from keras_tuner import BayesianOptimization"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import pandas as pd\n","from text_classification.paths import DATA_DIR, MODELS_DIR\n","from pathlib import Path\n","from collections import Counter\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from IPython.display import display\n","from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import text_classification.config as cfg\n","import tensorflow as tf\n","from tensorflow.keras import layers, models, regularizers\n","from tensorflow.keras.preprocessing import image_dataset_from_directory\n","\n","import re\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from textblob import TextBlob\n","from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n","import pandas as pd\n","\n","# Download stopwords and lemmatizer data if necessary\n","# nltk.download('stopwords')\n","# nltk.download('wordnet')\n","\n","%matplotlib inline"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["import tensorflow as tf\n","from keras import backend as K"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["def f1_score(y_true, y_pred):\n","    # Convert predictions to binary values\n","    y_pred_bin = K.round(y_pred)\n","\n","    # Calculate true positives, false positives, false negatives\n","    tp = K.sum(K.cast(y_true * y_pred_bin, 'float'), axis=0)\n","    fp = K.sum(K.cast((1 - y_true) * y_pred_bin, 'float'), axis=0)\n","    fn = K.sum(K.cast(y_true * (1 - y_pred_bin), 'float'), axis=0)\n","\n","    # Calculate precision and recall\n","    precision = tp / (tp + fp + K.epsilon())\n","    recall = tp / (tp + fn + K.epsilon())\n","\n","    # Calculate F1 score\n","    f1 = 2 * precision * recall / (precision + recall + K.epsilon())\n","    return K.mean(f1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Check if MPS is available\n","if tf.config.list_physical_devices('GPU'):\n","    print(\"MPS backend is available and will be used.\")\n","else:\n","    print(\"MPS backend is not available. Using CPU instead.\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load data \n","data = pd.read_csv(DATA_DIR / 'historical_data.csv')\n","\n","# train overview\n","data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Basic information about the dataset\n","data.info()\n","\n","# Check the proportions of the target variable\n","data['Recommended.IND'].value_counts(normalize=True)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# Rename the columns\n","data = (\n","    data\n","    .rename(columns={'Recommended.IND': 'recommended', 'Review.Text': 'review'})\n",")"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["# Load from the text file negations\n","with open(Path(DATA_DIR / 'negations.txt'), 'r') as file:\n","    negations = [line.strip() for line in file]"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["# Initialize stop words and lemmatizer\n","stop_words = set(stopwords.words('english'))\n","lemmatizer = WordNetLemmatizer()\n","\n","# Initialize the VADER sentiment analyzer\n","analyzer = SentimentIntensityAnalyzer()\n","\n","# Function to detect and transform negated phrases\n","def handle_negations(text):\n","    words = text.split()  # Tokenize the text\n","    transformed_words = []\n","    negate = False\n","\n","    for i, word in enumerate(words):\n","        # If negation detected, append \"not\" to the following word\n","        if word in negations and i + 1 < len(words):\n","            transformed_words.append(f\"not_{words[i + 1]}\")\n","            negate = True\n","            i += 1  # Skip the next word as it's combined with negation\n","        elif negate:\n","            transformed_words.append(f\"not_{word}\")\n","            negate = False\n","        else:\n","            transformed_words.append(word)\n","    return ' '.join(transformed_words)\n","\n","# Function to handle repeated characters (like \"soooo good\" -> \"soo good\")\n","def handle_repeated_characters(word):\n","    return re.sub(r'(.)\\1+', r'\\1\\1', word)\n","\n","# Preprocessing function for sentiment analysis\n","def clean_text(text):\n","    # Convert text to lowercase\n","    text = text.lower()\n","    \n","    # Remove all punctuation\n","    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n","    \n","    # Split the text into words\n","    words = text.split()\n","    \n","    # Remove stopwords, but keep negations and important words like \"but\", \"very\", etc.\n","    words = [word for word in words if word not in stop_words or word in negations]\n","    \n","    # Handle negations by concatenating them with the following word\n","    neg_handled_text = handle_negations(' '.join(words))\n","    \n","    # Tokenize the processed text after handling negations\n","    words_processed = neg_handled_text.split()\n","    \n","    # Lemmatize words\n","    lemmatized_words = [lemmatizer.lemmatize(word) for word in words_processed]\n","    \n","    # Handle repeated characters\n","    lemmatized_words = [handle_repeated_characters(word) for word in lemmatized_words]\n","    \n","    # Join the cleaned words back into a string\n","    cleaned_text = ' '.join(lemmatized_words)\n","    \n","    return cleaned_text\n","\n","\n","\n","# Apply the cleaning function to the review text column\n","data['cleaned_review'] = data['review'].apply(clean_text)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Calculate the number of characters per line in the 'review' and 'cleaned_review' columns\n","data['review_char_count'] = data['review'].str.len()\n","data['cleaned_review_char_count'] = data['cleaned_review'].str.len()\n","\n","# Print the first few rows to verify the new columns\n","data.head()"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["X = data.drop(columns=['recommended', 'review', 'review_char_count', 'cleaned_review_char_count'])\n","y = data['recommended']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X.shape, y.shape"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["# Split the data into training and validation sets (80% train, 20% validation)\n","X_train, X_val, y_train, y_val = train_test_split(\n","    X, \n","    y, \n","    test_size=0.20, \n","    stratify=y,\n","    random_state=123\n",")"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["# Ensure X_train and X_val contain the text from the 'cleaned_review' column\n","X_train_text = X_train['cleaned_review'].tolist()  # Convert the column to a list of strings\n","X_val_text = X_val['cleaned_review'].tolist()      # Convert validation set\n","\n","# Tokenization parameters\n","max_words = 2500\n","max_length = 60  # Set the max_length to 60 to ensure each review has 60 words\n","\n","# Initialize the tokenizer and fit it on the training data (on the actual text)\n","tokenizer = Tokenizer(num_words=max_words)\n","tokenizer.fit_on_texts(X_train_text)  # Fit tokenizer on the list of cleaned reviews\n","\n","# Convert the training and validation sets to sequences of integers\n","X_train_seq = tokenizer.texts_to_sequences(X_train_text)\n","X_val_seq = tokenizer.texts_to_sequences(X_val_text)\n","\n","# Pad the sequences to ensure uniform input length\n","X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding='post')\n","X_val_pad = pad_sequences(X_val_seq, maxlen=max_length, padding='post')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["len(X_train_pad), len(X_val_pad), len(y_train), len(y_val)  "]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["# 8. Load GloVe embeddings and create the embedding matrix\n","glove_file = Path(DATA_DIR / 'glove.6B.100d.txt')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["embedding_dim = 100  # For GloVe 100D\n","max_words = 2500  # Should match the tokenizer num_words\n","\n","# Initialize dictionary to store GloVe embeddings\n","embeddings_index = {}\n","\n","# Read the GloVe file and load into embeddings_index\n","with open(glove_file, 'r', encoding='utf8') as f:\n","    for line in f:\n","        values = line.split()\n","        word = values[0]  # First element is the word\n","        coefs = np.asarray(values[1:], dtype='float32')  # The rest are the embedding coefficients\n","        embeddings_index[word] = coefs\n","\n","# Get the word index from the tokenizer\n","word_index = tokenizer.word_index\n","\n","# Limit vocab size to max_words (2500)\n","vocab_size = min(max_words, len(word_index) + 1)\n","\n","# Initialize the embedding matrix with zeros (shape = (max_words, embedding_dim))\n","embedding_matrix = np.zeros((vocab_size, embedding_dim))\n","\n","# Fill the embedding matrix with GloVe embeddings for words found in the GloVe\n","# For words not found in GloVe, keep the initialized zeros or initialize randomly\n","for word, i in word_index.items():\n","    if i < max_words:  # Only consider the top max_words (2500)\n","        embedding_vector = embeddings_index.get(word)\n","        if embedding_vector is not None:\n","            embedding_matrix[i] = embedding_vector  # Set GloVe embedding for this word\n","        else:\n","            # Randomly initialize embeddings for words not found in GloVe\n","            embedding_matrix[i] = np.random.uniform(-0.05, 0.05, embedding_dim)  # Random small values\n","\n","# Print the shape of the final embedding matrix\n","print(f\"Final embedding matrix shape: {embedding_matrix.shape}\")\n"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["# Assuming y_train contains the training labels\n","classes = np.unique(y_train)\n","class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n","class_weight_dict = dict(zip(classes, class_weights))\n","\n","# Define aditional class weights\n","class_weight_options = [\n","    class_weight_dict,\n","    {0: 3.0, 1: 0.5},\n","    {0: 4.0, 1: 1.0},\n","    {0: 5.00, 1: 2.00}\n","]"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["def build_model(hp):\n","    model = Sequential()\n","\n","    # Embedding layer using combined embeddings (GloVe + random embeddings for OOV)\n","    model.add(Embedding(input_dim=cfg.MAX_FREQ_WORDS,   # Should match the tokenizer's vocab size or max_words\n","                        output_dim=cfg.MAX_EMBEDDINGS_DIM,  # 100 for GloVe 100D\n","                        weights=[embedding_matrix],  # Combined embedding matrix (GloVe + random)\n","                        input_length=60,  # Set input length (max_length of your padded sequences)\n","                        trainable=True))  # Set to True if you want to fine-tune embeddings\n","    \n","    # Choose weight initializer\n","    weight_initializer = hp.Choice('weight_initializer', values=['he_normal', 'glorot_uniform', 'lecun_normal'])\n","\n","    # 1D Convolutional Layer with L2 regularization and padding\n","    model.add(Conv1D(filters=hp.Int('conv_filters', min_value=16, max_value=264, step=16),\n","                     kernel_size=hp.Choice('conv_kernel_size', values=[3, 5]),\n","                     activation='relu',\n","                     padding='same',\n","                     kernel_regularizer=l2(hp.Float('l2_lambda', min_value=1e-5, max_value=1e-2, sampling='LOG')),\n","                     kernel_initializer=weight_initializer))\n","    model.add(MaxPooling1D(pool_size=2))\n","\n","    # LSTM layer with L2 regularization\n","    model.add(LSTM(units=hp.Int('lstm_units', min_value=16, max_value=264, step=16),\n","                   return_sequences=True,\n","                   kernel_regularizer=l2(hp.Float('l2_lambda', min_value=1e-5, max_value=1e-2, sampling='LOG')),\n","                   kernel_initializer=weight_initializer))\n"," \n","    # GRU layer\n","    model.add(Bidirectional(GRU(units=hp.Int('gru_units', min_value=16, max_value=264, step=16),\n","                                return_sequences=False)))\n","\n","    # Dense layer\n","    model.add(Dense(units=hp.Int('dense_units', min_value=16, max_value=264, step=16),\n","                    activation='relu',\n","                    kernel_regularizer=l2(hp.Float('l2_lambda', min_value=1e-5, max_value=1e-2, sampling='LOG')),\n","                    kernel_initializer=weight_initializer))\n","    model.add(Dropout(rate=hp.Float('dropout_rate', min_value=0.2, max_value=0.7, step=0.1)))\n","\n","    # Output layer for binary classification\n","    model.add(Dense(1, activation='sigmoid', kernel_initializer=weight_initializer))\n","\n","    # Choose optimizer\n","    optimizer = hp.Choice('optimizer', values=['adam', 'rmsprop', 'sgd'])\n","    if optimizer == 'adam':\n","        optimizer_instance = Adam(learning_rate=hp.Float('learning_rate', min_value=1e-5, max_value=1e-2, sampling='LOG'))\n","    elif optimizer == 'rmsprop':\n","        optimizer_instance = RMSprop(learning_rate=hp.Float('learning_rate', min_value=1e-5, max_value=1e-2, sampling='LOG'))\n","    elif optimizer == 'sgd':\n","        optimizer_instance = SGD(learning_rate=hp.Float('learning_rate', min_value=1e-5, max_value=1e-2, sampling='LOG'))\n","\n","    # Dynamically select class weights\n","    class_weight_index = hp.Choice('class_weight_index', [0, 1, 2, 3, 4])\n","    class_weight = class_weight_options[class_weight_index]\n","\n","    # Compile the model\n","    model.compile(optimizer=optimizer_instance,\n","                  loss='binary_crossentropy',\n","                  metrics=['accuracy'])\n","\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Initialize the BayesianOptimization tuner\n","tuner = BayesianOptimization(\n","    build_model,\n","    objective='val_accuracy',\n","    max_trials=10,\n","    executions_per_trial=1,\n","    directory= Path(MODELS_DIR / 'tuner_random'),\n","    project_name='bayes_opt'\n",")\n","\n","# Define the search space\n","tuner.search_space_summary()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Use the tuner to search the hyperparameter space, but now pass the class weights dynamically inside the search loop\n","tuner.search(\n","    X_train_pad, y_train,\n","    epochs=15,\n","    validation_data=(X_val_pad, y_val),\n","    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)],\n","    # Class weights will be selected dynamically within the model, no need to pass here\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get the best hyperparameters after tuning\n","best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n","\n","# Retrieve the best class weight index\n","best_class_weight_index = best_hps.get('class_weight_index')\n","\n","# Retrieve the corresponding class weight dictionary\n","best_class_weight = class_weight_options[best_class_weight_index]\n","\n","print(f\"The best class weight is: {best_class_weight}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from tensorflow.keras.callbacks import EarlyStopping\n","\n","y_train = np.array(y_train)\n","y_val = np.array(y_val)\n","\n","# Build the model using the best hyperparameters\n","model_2 = build_model_1(best_hps)\n","\n","# Define early stopping callback to avoid overfitting\n","early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n","\n","# Train the model with the best hyperparameters and appropriate class weights\n","history = model_2.fit(\n","    X_train_pad, y_train,\n","    epochs=50,\n","    validation_data=(X_val_pad, y_val),\n","    class_weight=class_weight_dict, \n","    callbacks=[early_stopping]\n",")\n","\n","# Display the model summary\n","model_2.summary()"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["# Save the trained model\n","model_2.save(MODELS_DIR / 'lstm_emb.keras') "]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["# Load the saved model\n","tc2_model = tf.keras.models.load_model(MODELS_DIR / 'lstm_emb.keras')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Binary classification\n","y_pred_prob = tc2_model.predict(X_val_pad)\n","y_pred = (y_pred_prob > 0.5).astype(int)  # Convert probabilities to class labels using a threshold of 0.5"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.metrics import confusion_matrix, classification_report\n","\n","# Calculate the confusion matrix\n","conf_matrix = confusion_matrix(y_val, y_pred)\n","\n","# Print the classification report\n","print(classification_report(y_val, y_pred))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class_counts = np.bincount(y_val)\n","# Print the number of occurrences of each class\n","print(f\"Number of 0s: {class_counts[0]}\")\n","print(f\"Number of 1s: {class_counts[1]}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np\n","from sklearn.metrics import confusion_matrix, classification_report\n","\n","# Assuming y_val and y_pred are already defined\n","# y_val: true labels\n","# y_pred: predicted labels\n","\n","# Calculate the confusion matrix\n","conf_matrix = confusion_matrix(y_val, y_pred)\n","\n","# Plot the confusion matrix using Seaborn\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='viridis')\n","plt.xlabel('Predicted')\n","plt.ylabel('Actual')\n","plt.title('Confusion Matrix')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import plotly.graph_objects as go\n","\n","# Create traces for training and validation loss\n","fig = go.Figure()\n","\n","fig.add_trace(go.Scatter(\n","    x=list(range(1, len(history.history['loss']) + 1)),\n","    y=history.history['loss'],\n","    mode='lines+markers',\n","    name='Training Loss'\n","))\n","\n","fig.add_trace(go.Scatter(\n","    x=list(range(1, len(history.history['val_loss']) + 1)),\n","    y=history.history['val_loss'],\n","    mode='lines+markers',\n","    name='Validation Loss'\n","))\n","\n","# Add titles and labels\n","fig.update_layout(\n","    title='Training and Validation Loss',\n","    xaxis_title='Epochs',\n","    yaxis_title='Loss',\n","    legend=dict(x=0, y=1),\n","    hovermode='x unified'\n",")\n","\n","# Show the plot\n","fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create traces for training and validation accuracy\n","fig_accuracy = go.Figure()\n","\n","fig_accuracy.add_trace(go.Scatter(\n","    x=list(range(1, len(history.history['accuracy']) + 1)),\n","    y=history.history['accuracy'],\n","    mode='lines+markers',\n","    name='Training Accuracy'\n","))\n","\n","fig_accuracy.add_trace(go.Scatter(\n","    x=list(range(1, len(history.history['val_accuracy']) + 1)),\n","    y=history.history['val_accuracy'],\n","    mode='lines+markers',\n","    name='Validation Accuracy'\n","))\n","\n","# Add titles and labels for accuracy plot\n","fig_accuracy.update_layout(\n","    title='Training and Validation Accuracy',\n","    xaxis_title='Epochs',\n","    yaxis_title='Accuracy',\n","    legend=dict(x=0, y=1),\n","    hovermode='x unified'\n",")\n","\n","# Show the accuracy plot\n","fig_accuracy.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import plotly.graph_objects as go\n","from plotly.subplots import make_subplots\n","\n","# Create subplots: 1 row, 2 columns\n","fig = make_subplots(rows=1, cols=2, subplot_titles=('Loss Evolution', 'Accuracy Evolution'))\n","\n","# Add traces for training and validation loss (left subplot)\n","fig.add_trace(go.Scatter(\n","    x=list(range(1, len(history.history['loss']) + 1)),\n","    y=history.history['loss'],\n","    mode='lines+markers',\n","    name='Train Loss'\n","), row=1, col=1)\n","\n","fig.add_trace(go.Scatter(\n","    x=list(range(1, len(history.history['val_loss']) + 1)),\n","    y=history.history['val_loss'],\n","    mode='lines+markers',\n","    name='Val Loss'\n","), row=1, col=1)\n","\n","# Add traces for training and validation accuracy (right subplot)\n","fig.add_trace(go.Scatter(\n","    x=list(range(1, len(history.history['accuracy']) + 1)),\n","    y=history.history['accuracy'],\n","    mode='lines+markers',\n","    name='Train Acc'\n","), row=1, col=2)\n","\n","fig.add_trace(go.Scatter(\n","    x=list(range(1, len(history.history['val_accuracy']) + 1)),\n","    y=history.history['val_accuracy'],\n","    mode='lines+markers',\n","    name='Val Acc'\n","), row=1, col=2)\n","\n","# Update layout for the entire figure\n","fig.update_layout(\n","    width=1000,  # Set the width of the figure\n","    height=400,\n","    showlegend=True,\n","    legend=dict(x=1.05, y=1, orientation='v'),  # Place legend on the right\n","    hovermode='x unified',\n","    plot_bgcolor='rgba(0,0,0,0)',  # Remove background\n","    paper_bgcolor='rgba(0,0,0,0)',  # Remove background\n","    font=dict(color='black')  # Set font color to black\n",")\n","\n","# Update x-axis and y-axis titles for each subplot\n","fig.update_xaxes(title_text='Epochs', row=1, col=1, title_font=dict(color='black'), tickfont=dict(color='black'))\n","fig.update_yaxes(title_text='Loss', row=1, col=1, title_font=dict(color='black'), tickfont=dict(color='black'))\n","fig.update_xaxes(title_text='Epochs', row=1, col=2, title_font=dict(color='black'), tickfont=dict(color='black'))\n","fig.update_yaxes(title_text='Accuracy', row=1, col=2, title_font=dict(color='black'), tickfont=dict(color='black'))\n","\n","# Show the plot\n","fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","# Calculate the ROC curve\n","fpr, tpr, thresholds = roc_curve(y_val, y_pred_prob)\n","roc_auc = auc(fpr, tpr)\n","\n","# Create the ROC curve plot\n","fig_roc = go.Figure()\n","\n","fig_roc.add_trace(go.Scatter(\n","    x=fpr,\n","    y=tpr,\n","    mode='lines',\n","    name=f'ROC curve (area = {roc_auc:.2f})',\n","    line=dict(color='red', width=2)\n","))\n","\n","# Add a diagonal line representing a random classifier\n","fig_roc.add_trace(go.Scatter(\n","    x=[0, 1],\n","    y=[0, 1],\n","    mode='lines',\n","    name='Random Classifier',\n","    line=dict(color='gray', width=2, dash='dash')\n","))\n","\n","# Update layout for the ROC curve plot\n","fig_roc.update_layout(\n","    title='Receiver Operating Characteristic (ROC) Curve',\n","    xaxis_title='False Positive Rate',\n","    yaxis_title='True Positive Rate',\n","    width=1000,  # Set the width of the figure\n","    height=400,  # Set the height of the figure\n","    showlegend=True,\n","    legend=dict(x=1.05, y=1, orientation='v'),  # Place legend on the right\n","    hovermode='x unified',\n","    plot_bgcolor='rgba(0,0,0,0)',  # Remove background\n","    paper_bgcolor='rgba(0,0,0,0)',  # Remove background\n","    font=dict(color='black')  # Set font color to black\n",")\n","\n","# Show the ROC curve plot\n","fig_roc.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Generate predictions on the validation set\n","y_val_pred = (model.predict(X_val_pad) > 0.5).astype(\"int32\")\n","\n","# Print confusion matrix and classification report\n","conf_matrix = confusion_matrix(y_val, y_val_pred)\n","class_report = classification_report(y_val, y_val_pred)\n","\n","print(\"Confusion Matrix:\")\n","print(conf_matrix)\n","print(\"\\nClassification Report:\")\n","print(class_report)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["conf_matrix"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np\n","\n","# Create a heatmap\n","plt.figure(figsize=(4, 2))\n","\n","# Create the heatmap with no color bar and smaller annotation size\n","ax = sns.heatmap(conf_matrix, annot=True,\n","                 fmt='d',\n","                 cmap='viridis', \n","                 xticklabels=x_labels,\n","                 yticklabels=y_labels,\n","                 cbar=False, \n","                 annot_kws={\"size\": 8})\n","\n","# Customize the font size of the tick labels\n","ax.tick_params(axis='x', labelsize=8)\n","ax.tick_params(axis='y', labelsize=8)\n","\n","# Add labels\n","plt.xlabel('Predicted', size = 10, labelpad=10)\n","plt.ylabel('Actual', size=8, labelpad=8)\n","\n","# Move the x-axis label to the top\n","plt.gca().xaxis.set_label_position('top')\n","plt.gca().xaxis.tick_top()\n","\n","# Show the plot\n","plt.show()"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["# Clear the TensorFlow Keras session\n","tf.keras.backend.clear_session()"]}],"metadata":{"kernelspec":{"display_name":"text-classification-B9jIwe8H-py3.11","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":2}
